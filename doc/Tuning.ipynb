{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning the Discriminator\n",
    "\n",
    "*This documentation is still being written.*\n",
    "\n",
    "It is possible to tune the Ringer discriminator both on standalone and on the GRID. The latter only applies if you have installed the TuningTools within a, at least, Tier 3 site.\n",
    "\n",
    "In order to run a standalone tuning, you can both run: \n",
    "\n",
    "- [Recommended] [use the executable](#Using-the-tuning-shell-command); or\n",
    "- a [python script](#Running-through-python-script). \n",
    "\n",
    "If you have access to the GRID, you will need to run the [shell command to upload the job](#Running-the-GRID-dispatch-tuning-command).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Standalone\n",
    "\n",
    "Most of the TuningTools functionalities can be accessed through python scripts or a shell command. The executable is the recommended way for running a standalone job.\n",
    "\n",
    "## Using the tuning shell command\n",
    "\n",
    "This is the recommended way for interacting with the tunning job. You will have to specify the configuration you want to use, such as the pre-processing, the Cross-Validation object and the discriminator parameters to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running through python script\n",
    "\n",
    "However, you can directly access the `TuningJob` class, and call it using a python script. The __call__ method documentation cover all available options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method __call__ in module TuningTools.TuningJob:\n",
      "\n",
      "__call__(self, dataLocation, **kw) unbound TuningTools.TuningJob.TuningJob method\n",
      "    Run discrimination tuning for input data created via CreateData.py\n",
      "    Arguments:\n",
      "      - dataLocation: A string containing a path to the data file written\n",
      "        by CreateData.py\n",
      "    Mutually exclusive optional arguments: Either choose the cross (x) or\n",
      "      circle (o) of the following block options.\n",
      "     -------\n",
      "      x crossValid [CrossValid( nSorts=50, nBoxes=10, nTrain=6, nValid=4, \n",
      "                                seed=crossValidSeed )]:\n",
      "        The cross-validation sorts object. The files can be generated using a\n",
      "        CreateConfFiles instance which can be accessed via command line using\n",
      "        the createTuningJobFiles.py script.\n",
      "      x crossValidSeed [None]: Only used when not specifying the crossValid option.\n",
      "        The seed is used by the cross validation random sort generator and\n",
      "        when not specified or specified as None, time is used as seed.\n",
      "      o crossValidFile: The cross-validation file path, pointing to a file\n",
      "        created with the create tuning job files\n",
      "     -------\n",
      "      x confFileList [None]: A python list or a comma separated list of the\n",
      "        root files containing the configuration to run the jobs. The files can\n",
      "        be generated using a CreateConfFiles instance which can be accessed via\n",
      "        command line using the createTuningJobFiles.py script.\n",
      "      o neuronBoundsCol [MatlabLoopingBounds(5,5)]: A LoopingBoundsCollection\n",
      "        range where the the neural network should loop upon.\n",
      "      o sortBoundsCol [PythonLoopingBounds(50)]: A LoopingBoundsCollection\n",
      "        range for the sorts to use from the crossValid object.\n",
      "      o initBoundsCol [PythonLoopingBounds(100)]: A LoopingBoundsCollection\n",
      "        range for the initialization numbers to be ran on this tuning job.\n",
      "        The neuronBoundsCol, sortBoundsCol, initBoundsCol will be synchronously\n",
      "        looped upon, that is, all the first collection information upon those \n",
      "        variables will be used to feed the first job configuration, all of \n",
      "        those second collection information will be used to feed the second job \n",
      "        configuration, and so on...\n",
      "        In the case you have only one job configuration, it can be input as\n",
      "        a single LoopingBounds instance, or values that feed the LoopingBounds\n",
      "        initialization. In the last case, the neuronBoundsCol will be used\n",
      "        to feed a MatlabLoopingBounds, and the sortBoundsCol together with\n",
      "        initBoundsCol will be used to feed a PythonLoopingBounds.\n",
      "        For instance, if you use neuronBoundsCol set to [5,2,11], it will \n",
      "        loop upon the list [5,7,9,11], while if this was set to sortBoundsCol,\n",
      "        it would generate [5,7,9].\n",
      "     -------\n",
      "      x ppFileList [None]: A python list or a comma separated list of the\n",
      "        root files containing the pre-processing chain to apply into \n",
      "        input space and obtain the pattern space. The files can be generated\n",
      "        using a CreateConfFiles instance which is accessed via command\n",
      "        line using the createTuningJobFiles.py script.\n",
      "        The ppFileList must have a file for each of the configuration list \n",
      "        defined, that is, one pre-processing chain for each one of the \n",
      "        neuron/sort/init bounds collection. When only one ppFile is defined and\n",
      "        the configuration list has size greater than one, the pre-processing\n",
      "        chain will be copied for being applied on the other bounds.\n",
      "      o ppCol [PreProcChain( Norm1() )]: A PreProcCollection with the\n",
      "        PreProcChain instances to be applied to each of the configuration\n",
      "        ranges chosen by the above configurations.\n",
      "        The ppCol must have a file for each of the configuration list \n",
      "        defined, that is, one pre-processing chain for each one of the \n",
      "        neuron/sort/init bounds collection. When only one ppFile is defined\n",
      "        and the configuration list has size greater than one, the\n",
      "        pre-processing chain will be copied for being applied on the other\n",
      "        bounds.\n",
      "     -------\n",
      "    Optional arguments:\n",
      "      - operationLevel [None]: The discriminator operation level. When set to\n",
      "          None, the operation level will be retrieved from the tuning data\n",
      "          file. For now, this is only used to set the default operation targets\n",
      "          on Loose and Tight tunings.\n",
      "      - etBins [None]: The et bins to use within this job. When not specified,\n",
      "        all bins available on the file will be tuned separately.\n",
      "      - etaBins [None]: The eta bins to use within this job. When not specified,\n",
      "        all bins available on the file will be tuned separately.\n",
      "      - tuneOperationTargets [['Loose', 'Pd' , #looseBenchmarkRef],\n",
      "                              ['Medium', 'SP'],\n",
      "                              ['Tight', 'Pf' , #tightBenchmarkRef]]\n",
      "          The tune operation targets which should be used for this tuning\n",
      "          job. The strings inputs must be part of the ReferenceBenchmark\n",
      "          enumeration.\n",
      "          Instead of an enumeration string (or the enumeration itself),\n",
      "          you can set it directly to a value, e.g.: \n",
      "            [['Loose97', 'Pd', .97,],['Tight005','Pf',.005]]\n",
      "          This can also be set using a string, e.g.:\n",
      "            [['Loose97','Pd' : '.97'],['Tight005','Pf','.005']]\n",
      "          , which may contain a percentage symbol:\n",
      "            [['Loose97','Pd' : '97%'],['Tight005','Pf','0.5%']]\n",
      "          When set to None, the Pd and Pf will be set to the value of the\n",
      "          benchmark correspondent to the operation level set.\n",
      "      - compress [True]: Whether to compress file or not.\n",
      "      - level [loggingLevel.INFO]: The logging output level.\n",
      "      - outputFileBase ['nn.tuned']: The tuning outputFile starting string.\n",
      "          It will also contain a custom string representing the configuration\n",
      "          used to tune the discriminator.\n",
      "      - showEvo (TuningWrapper prop) [50]: The number of iterations wher\n",
      "          performance is shown (used as a boolean on ExMachina).\n",
      "      - maxFail (TuningWrapper prop) [50]: Maximum number of failures\n",
      "          tolerated failing to improve performance over validation dataset.\n",
      "      - epochs (TuningWrapper prop) [1000]: Number of iterations where\n",
      "          the tuning algorithm can run the optimization.\n",
      "      - doPerf (TuningWrapper prop) [True]: Whether we should run performance\n",
      "          testing under convergence conditions, using test/validation dataset\n",
      "          and also estimate operation condition.\n",
      "      - maxFail (TuningWrapper prop) [50]: Number of epochs which failed to improve\n",
      "          validation efficiency. When reached, the tuning process is stopped.\n",
      "      - batchSize (TuningWrapper prop) [number of observations of the class\n",
      "          with the less observations]: Set the batch size used during tuning.\n",
      "      - algorithmName (TuningWrapper prop) [resilient back-propgation]: The\n",
      "          tuning method to use.\n",
      "      - networkArch (ExMachina prop) ['feedforward']: the neural network\n",
      "          architeture to use.\n",
      "      - costFunction (ExMachina prop) ['sp']: the cost function used by ExMachina\n",
      "      - shuffle (ExMachina prop) [True]: Whether to shuffle datasets while\n",
      "        training.\n",
      "      - seed (FastNet prop) [None]: The seed to be used by the tuning\n",
      "          algorithm.\n",
      "      - doMultiStop (FastNet prop) [True]: Tune classifier using P_D, P_F and\n",
      "        SP when set to True. Uses only SP when set to False.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from TuningTools.TuningJob import TuningJob\n",
    "help(TuningJob.__call__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning on the GRID\n",
    "\n",
    "Running the tuning job on the GRID will require more steps. It isn't possible to configure each one of the jobs via the shell, so we will need to create one configuration file for each one of the jobs we will want the GRID to run, as it seems that there is not other way to tell the panda pilot how to divide the job subsets.\n",
    "\n",
    "In order to do so, we will first have to [create the configuration data](#Creating-configuration-data) and afterwards export it to be [available on the GRID](#Exporting-data-to-the-GRID). Only after it will be possible to [dispach the job to the GRID](#Running-the-GRID-dispatch-tuning-command).\n",
    "\n",
    "However, the GRID computational power will make possible to tune the discriminators much faster if the computational effort needed is large :).\n",
    "\n",
    "## Creating configuration data\n",
    "\n",
    "\n",
    "\n",
    "## Exporting data to the GRID\n",
    "\n",
    "\n",
    "## Running the GRID dispatch tuning command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Standalone job via python script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Still to be written*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load ../scripts/skeletons/time_test.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# TODO Improve skeleton documentation\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "start = timer()\n",
    "\n",
    "DatasetLocationInput = '/afs/cern.ch/work/j/jodafons/public/validate_tuningtool/mc14_13TeV.147406.129160.sgn.offLikelihood.bkg.truth.trig.e24_lhmedium_L1EM20VH_etBin_0_etaBin_0.npz'\n",
    "\n",
    "#try:\n",
    "from RingerCore.Logger import Logger, LoggingLevel\n",
    "mainLogger = Logger.getModuleLogger(__name__)\n",
    "mainLogger.info(\"Entering main job.\")\n",
    "\n",
    "from TuningTools.TuningJob import TuningJob\n",
    "tuningJob = TuningJob()\n",
    "\n",
    "from TuningTools.PreProc import *\n",
    "\n",
    "basepath = '/afs/cern.ch/work/j/jodafons/public'\n",
    "\n",
    "tuningJob( DatasetLocationInput, \n",
    "           neuronBoundsCol = [15, 15], \n",
    "           sortBoundsCol = [0, 1],\n",
    "           initBoundsCol = 5, \n",
    "           #confFileList = basepath + '/user.wsfreund.config.nn5to20_sorts50_1by1_inits100_100by100/job.hn0015.s0040.il0000.iu0099.pic.gz',\n",
    "           #ppFileList = basepath+'/user.wsfreund.Norm1/ppFile_pp_Norm1.pic.gz',\n",
    "           #crossValidFile = basepath+'/user.wsfreund.CrossValid.50Sorts.seed_0/crossValid.pic.gz',\n",
    "           epochs = 100,\n",
    "           showEvo = 0,\n",
    "           #algorithmName= 'rprop',\n",
    "           #doMultiStop = True,\n",
    "           #doPerf = True,\n",
    "           maxFail = 100,\n",
    "           #seed = 0,\n",
    "           ppCol = PreProcCollection( PreProcChain( MapStd() ) ),\n",
    "           crossValidSeed = 66,\n",
    "           level = LoggingLevel.DEBUG )\n",
    "\n",
    "mainLogger.info(\"Finished.\")\n",
    "\n",
    "end = timer()\n",
    "\n",
    "print 'execution time is: ', (end - start)      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script type=\"text/javascript\">\n",
    "    show=true;\n",
    "    function toggle(){\n",
    "        if (show){\n",
    "            $('div.input').hide();\n",
    "        }else{\n",
    "            $('div.input').show();\n",
    "        }\n",
    "        show = !show\n",
    "    }\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n",
    "</script>\n",
    "<a href=\"javascript:toggle()\" target=\"_self\"></a>"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.4"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
